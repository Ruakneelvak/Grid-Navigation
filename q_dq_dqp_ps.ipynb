{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf67e06-4f8f-4d61-847a-4bf8223cf5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Grid Representation:\n",
      "['Start', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Blocked', 'Empty']\n",
      "['Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Blocked', 'Empty']\n",
      "['Empty', 'Empty', 'Empty', 'Empty', 'Blocked', 'Empty', 'Empty', 'Empty']\n",
      "['Blocked', 'Blocked', 'Blocked', 'Blocked', 'Empty', 'Empty', 'Blocked', 'Empty']\n",
      "['Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Blocked', 'Empty']\n",
      "['Empty', 'Blocked', 'Empty', 'Blocked', 'Blocked', 'Blocked', 'Blocked', 'Blocked']\n",
      "['Empty', 'Empty', 'Empty', 'Blocked', 'Empty', 'Empty', 'Empty', 'Empty']\n",
      "['Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Empty', 'Target']\n",
      "Selected Algorithm: Q-learning\n",
      "Optimal Path (Q-learning): [(0, 0), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 5), (3, 5), (3, 4), (4, 4), (4, 3), (4, 2), (5, 2), (6, 2), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7)]\n",
      "Number of steps taken by agent to reach target (Q-learning): 20\n",
      "Selected Algorithm: DynaQ+\n",
      "Optimal Path (Dyna-Q+): [(0, 0), (0, 1), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (2, 5), (3, 5), (3, 4), (4, 4), (4, 3), (4, 2), (5, 2), (6, 2), (7, 2), (7, 3), (7, 4), (7, 5), (7, 6), (7, 7)]\n",
      "Number of steps taken by agent to reach target (Dyna-Q+): 20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random as r\n",
    "from matplotlib import style\n",
    "from PyQt5 import QtCore, QtGui, QtWidgets\n",
    "from PyQt5.QtWidgets import QLabel, QVBoxLayout, QWidget, QApplication\n",
    "\n",
    "class Grid:  # Grid\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self, n, blocked_tiles):\n",
    "        self.n = n\n",
    "        self.blocked_tiles = blocked_tiles\n",
    "        self.grid = np.zeros((n, n))\n",
    "\n",
    "        for tile in blocked_tiles:\n",
    "            self.grid[tile] = -1\n",
    "\n",
    "    def isValid(self, pos):\n",
    "        x, y = pos\n",
    "        return 0 <= x < self.n and 0 <= y < self.n and self.grid[x, y] != -1  # Tile is not blocked\n",
    "\n",
    "    # Get possible actions from a position\n",
    "    def get_possible_acts(self, pos):\n",
    "        x, y = pos\n",
    "        acts = []\n",
    "        for act, (dx, dy) in {'u': (-1, 0), 'd': (1, 0), 'l': (0, -1), 'r': (0, 1)}.items():\n",
    "            new_pos = (x + dx, y + dy)\n",
    "            if self.isValid(new_pos):\n",
    "                acts.append((act, new_pos))\n",
    "        return acts\n",
    "\n",
    "    # Reset the grid\n",
    "    def reset(self):\n",
    "        self.grid = np.zeros((self.n, self.n))  # Clear the grid\n",
    "        for tile in self.blocked_tiles:\n",
    "            self.grid[tile] = -1\n",
    "\n",
    "# Agent\n",
    "import heapq\n",
    "import numpy as np\n",
    "import random as r\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.2, planning_steps=5, kappa=0.2):\n",
    "        self.env = env\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration vs exploitation\n",
    "        self.planning_steps = planning_steps  # Number of planning steps for Dyna-Q\n",
    "        self.kappa = kappa\n",
    "        self.Q_steps_per_ep = []\n",
    "        self.dynaQ_steps_per_ep = []\n",
    "        self.dynaQPlus_steps_per_ep = []\n",
    "        self.PrioritizedSweeping_steps_per_ep = []\n",
    "        self.Qtable = {}\n",
    "        self.model = {}\n",
    "        self.time_since_visited = {}\n",
    "        self.priority_queue = []\n",
    "        for x in range(self.env.n):\n",
    "            for y in range(self.env.n):\n",
    "                if self.env.grid[x, y] != -1:  # Not blocked\n",
    "                    self.Qtable[(x, y)] = {act: 0 for act, _ in self.env.get_possible_acts((x, y))}\n",
    "                    self.model[(x, y)] = {act: None for act, _ in self.env.get_possible_acts((x, y))}\n",
    "                    self.time_since_visited[(x, y)] = {act: 0 for act, _ in self.env.get_possible_acts((x, y))}\n",
    "\n",
    "    def getAct(self, state):\n",
    "        if r.uniform(0, 1) < self.epsilon:\n",
    "            return r.choice(list(self.Qtable[state].keys()))  # Exploration\n",
    "        else:\n",
    "            return max(self.Qtable[state], key=self.Qtable[state].get)  # Exploitation\n",
    "\n",
    "    def incQ(self, state, action, reward, next_state):\n",
    "        if next_state not in self.Qtable:\n",
    "            return\n",
    "        best_next_action = max(self.Qtable[next_state], key=self.Qtable[next_state].get)  # maxQ'(s', a')\n",
    "        td_target = reward + self.gamma * self.Qtable[next_state][best_next_action]  # R(s,a) + gamma * Q(s, a)\n",
    "        td_error = td_target - self.Qtable[state][action]  # R(s,a) + gamma * Q(s, a) - Q(s, a)\n",
    "        self.Qtable[state][action] += self.alpha * td_error  # Q(s, a) = Q(s, a) + alpha * { R(s,a) + [gamma * Q(s, a)] - Q(s, a) }\n",
    "        return abs(td_error)\n",
    "\n",
    "    def train_Q(self, episodes=2000):\n",
    "        global q_learning_episodes, min_Path_Q\n",
    "        for ep in range(episodes):\n",
    "            state = (r.randint(0, self.env.n - 1), r.randint(0, self.env.n - 1))\n",
    "            while not self.env.isValid(state) or state == (x_T, y_T):\n",
    "                state = (r.randint(0, self.env.n - 1), r.randint(0, self.env.n - 1))\n",
    "\n",
    "            while state != (x_T, y_T):\n",
    "                action = self.getAct(state)\n",
    "                next_state = [new_state for act, new_state in self.env.get_possible_acts(state) if act == action][0]\n",
    "                reward = 1 if next_state == (x_T, y_T) else -(1/self.env.n)\n",
    "                self.incQ(state, action, reward, next_state)\n",
    "                state = next_state\n",
    "\n",
    "            # Check if the path is optimal\n",
    "            if ep == 1:\n",
    "                min_Path_Q = 1000\n",
    "            if ep > 2:\n",
    "                path = self.path(start)\n",
    "                if self.is_optimal_path(path) and ep > 2:\n",
    "                    if len(path) < min_Path_Q:\n",
    "                        q_learning_episodes = ep + 1\n",
    "                        min_Path_Q = len(path)\n",
    "            self.Q_steps_per_ep.append(len(self.path(start)) - 1)\n",
    "\n",
    "    def train_DynaQ(self, episodes=2000):\n",
    "        global dyna_q_episodes, min_Path_dynaQ\n",
    "        for ep in range(episodes):\n",
    "            while True:\n",
    "                state = (r.randint(0, self.env.n - 1), r.randint(0, self.env.n - 1))\n",
    "                if self.env.isValid(state) and state != (x_T, y_T):\n",
    "                    break\n",
    "\n",
    "            while state != (x_T, y_T):\n",
    "                action = self.getAct(state)\n",
    "                next_state = [new_state for act, new_state in self.env.get_possible_acts(state) if act == action][0]\n",
    "                reward = 1 if next_state == (x_T, y_T) else -(1/self.env.n)\n",
    "                self.incQ(state, action, reward, next_state)\n",
    "\n",
    "                # Update the model\n",
    "                self.model[state][action] = (next_state, reward)\n",
    "\n",
    "                # Planning steps\n",
    "                for _ in range(self.planning_steps):\n",
    "                    sim_state = r.choice(list(self.model.keys()))\n",
    "                    sim_action = r.choice(list(self.model[sim_state].keys()))\n",
    "                    sim_next_state, sim_reward = self.model[sim_state][sim_action] if self.model[sim_state][sim_action] else (sim_state, 0)\n",
    "                    self.incQ(sim_state, sim_action, sim_reward, sim_next_state)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Check if the path is optimal\n",
    "            if ep == 1:\n",
    "                min_Path_dynaQ = 1000\n",
    "            if ep > 2:\n",
    "                path = self.path(start)\n",
    "                if self.is_optimal_path(path):\n",
    "                    if len(path) < min_Path_dynaQ:\n",
    "                        dyna_q_episodes = ep + 1\n",
    "                        min_Path_dynaQ = len(path)\n",
    "            self.dynaQ_steps_per_ep.append(len(self.path(start)) - 1)\n",
    "\n",
    "    def train_DynaQPlus(self, episodes=2000):\n",
    "        global dyna_q_plus_episodes, min_Path_dynaQPlus\n",
    "        for ep in range(episodes):\n",
    "            while True:\n",
    "                state = (r.randint(0, self.env.n - 1), r.randint(0, self.env.n - 1))\n",
    "                if self.env.isValid(state) and state != (x_T, y_T):\n",
    "                    break\n",
    "\n",
    "            while state != (x_T, y_T):\n",
    "                action = self.getAct(state)\n",
    "                next_state = [new_state for act, new_state in self.env.get_possible_acts(state) if act == action][0]\n",
    "                reward = 1 if next_state == (x_T, y_T) else -(1/self.env.n)\n",
    "                self.incQ(state, action, reward, next_state)\n",
    "\n",
    "                # Update the model and time since visited\n",
    "                self.model[state][action] = (next_state, reward)\n",
    "                self.time_since_visited[state][action] = ep\n",
    "\n",
    "                # Planning steps with bonus reward\n",
    "                for _ in range(self.planning_steps):\n",
    "                    # Choose a state-action pair based on the bonus\n",
    "                    max_bonus = -float('inf')\n",
    "                    chosen_state = None\n",
    "                    chosen_action = None\n",
    "\n",
    "                    for s in self.Qtable:\n",
    "                        for a in self.Qtable[s]:\n",
    "                            bonus = self.kappa * np.sqrt(ep - self.time_since_visited[s][a])\n",
    "                            if self.Qtable[s][a] + bonus > max_bonus:\n",
    "                                max_bonus = self.Qtable[s][a] + bonus\n",
    "                                chosen_state = s\n",
    "                                chosen_action = a\n",
    "\n",
    "                    sim_next_state, sim_reward = self.model[chosen_state][chosen_action] if self.model[chosen_state][chosen_action] else (chosen_state, 0)\n",
    "                    self.incQ(chosen_state, chosen_action, sim_reward, sim_next_state)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Check if the path is optimal\n",
    "            if ep == 1:\n",
    "                min_Path_dynaQPlus = 1000\n",
    "            if ep > 2:\n",
    "                path = self.path(start)\n",
    "                if self.is_optimal_path(path):\n",
    "                    if len(path) < min_Path_dynaQPlus:\n",
    "                        dyna_q_plus_episodes = ep + 1\n",
    "                        min_Path_dynaQPlus = len(path)\n",
    "            self.dynaQPlus_steps_per_ep.append(len(self.path(start)) - 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def prioritize(self, state, action, priority):\n",
    "        heapq.heappush(self.priority_queue, (-priority, state, action))  # Push the priority with a negative sign to make it a max-heap\n",
    "\n",
    "    def update_priorities(self, state, action, td_error):\n",
    "        if td_error > self.kappa:  # Check if the td_error exceeds the threshold\n",
    "            self.prioritize(state, action, td_error)  # Add to priority queue if it does\n",
    "\n",
    "    def get_predecessors(self, state):\n",
    "        predecessors = []\n",
    "        for s in self.model:\n",
    "            for a in self.model[s]:\n",
    "                if self.model[s][a] is not None:  # Ensure the model entry is not None\n",
    "                    ns, _ = self.model[s][a]\n",
    "                    if ns == state:\n",
    "                        predecessors.append((s, a))\n",
    "        return predecessors\n",
    "\n",
    "\n",
    "    def train_PrioritizedSweeping(self, episodes=2000):\n",
    "        global prioritized_sweeping_episodes, min_Path_PrioritizedSweeping\n",
    "        for ep in range(episodes):\n",
    "            while True:\n",
    "                state = (r.randint(0, self.env.n - 1), r.randint(0, self.env.n - 1))\n",
    "                if self.env.isValid(state) and state != (x_T, y_T):\n",
    "                    break\n",
    "\n",
    "            while state != (x_T, y_T):\n",
    "                action = self.getAct(state)\n",
    "                next_state = [new_state for act, new_state in self.env.get_possible_acts(state) if act == action][0]\n",
    "                reward = 1 if next_state == (x_T, y_T) else -(1/self.env.n)\n",
    "                td_error = self.incQ(state, action, reward, next_state)\n",
    "\n",
    "                # Update the model\n",
    "                self.model[state][action] = (next_state, reward)\n",
    "                self.update_priorities(state, action, td_error)\n",
    "\n",
    "                # Planning steps with prioritized sweeping\n",
    "                for _ in range(self.planning_steps):\n",
    "                    if not self.priority_queue:\n",
    "                        break\n",
    "                    priority, sim_state, sim_action = heapq.heappop(self.priority_queue)\n",
    "                    priority = -priority  # Convert back to positive priority\n",
    "                    sim_next_state, sim_reward = self.model[sim_state][sim_action] if self.model[sim_state][sim_action] else (sim_state, 0)\n",
    "                    td_error = self.incQ(sim_state, sim_action, sim_reward, sim_next_state)\n",
    "                    self.update_priorities(sim_state, sim_action, td_error)\n",
    "\n",
    "                    for pred_state, pred_action in self.get_predecessors(sim_state):\n",
    "                        pred_td_error = self.incQ(pred_state, pred_action, sim_reward, sim_state)\n",
    "                        self.update_priorities(pred_state, pred_action, pred_td_error)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            # Check if the path is optimal\n",
    "            if ep == 1:\n",
    "                min_Path_PrioritizedSweeping = 1000\n",
    "            if ep > 2:\n",
    "                path = self.path(start)\n",
    "                if self.is_optimal_path(path):\n",
    "                    if len(path) < min_Path_PrioritizedSweeping:\n",
    "                        prioritized_sweeping_episodes = ep + 1\n",
    "                        min_Path_PrioritizedSweeping = len(path)\n",
    "            self.PrioritizedSweeping_steps_per_ep.append(len(self.path(start)) - 1)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def path(self, start):\n",
    "        if start not in self.Qtable:\n",
    "            return []\n",
    "        path = [start]\n",
    "        state = start\n",
    "        while state != (x_T, y_T):\n",
    "            if len(path) > self.env.n * self.env.n:\n",
    "                return path\n",
    "            if not self.Qtable[state]:  # If current state has no available actions in Q-table, returns the path found so far.\n",
    "                return path\n",
    "            action = max(self.Qtable[state], key=self.Qtable[state].get)  # Selects the action with the highest Q-value from the current state\n",
    "            next_states = [new_state for act, new_state in self.env.get_possible_acts(state) if act == action]  # Find possible next states for selected action from current state\n",
    "            if not next_states:  # Breaks loop if no valid next states\n",
    "                break\n",
    "            if next_states[0] == state:  # Breaks loop if next state = current state\n",
    "                break\n",
    "\n",
    "            state = next_states[0]\n",
    "            path.append(state)\n",
    "        return path\n",
    "\n",
    "    def is_optimal_path(self, path):\n",
    "        return path[-1] == (x_T, y_T)\n",
    "\n",
    "    def reset(self):\n",
    "        self.Qtable.clear()\n",
    "        self.model.clear()\n",
    "        self.env.reset()\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "            \n",
    "\n",
    "\n",
    "def find_start_of_optimal_solution(arr):\n",
    "    if not arr:\n",
    "        return -1  # Return -1 or handle the case where arr is empty\n",
    "\n",
    "    min_value = min(arr)\n",
    "    n = len(arr)\n",
    "\n",
    "    for i in range(n):\n",
    "        if arr[i] == min_value:\n",
    "            if all(x == min_value for x in arr[i:]):\n",
    "                return i\n",
    "    return -1\n",
    "\n",
    "class Ui_MainWindow(object):\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        MainWindow.resize(400, 200)\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "        \n",
    "        self.verticalLayout = QtWidgets.QVBoxLayout(self.centralwidget)\n",
    "        self.verticalLayout.setContentsMargins(20, 20, 20, 20)\n",
    "        self.verticalLayout.setSpacing(20)\n",
    "        self.verticalLayout.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        \n",
    "        self.label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label.setText(\"Select the Size of the Grid (n) :\")\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.verticalLayout.addWidget(self.label)\n",
    "\n",
    "        self.comboBox = QtWidgets.QComboBox(self.centralwidget)\n",
    "        self.comboBox.setObjectName(\"comboBox\")\n",
    "        for i in range(2, 11):\n",
    "            self.comboBox.addItem(str(i))\n",
    "        self.verticalLayout.addWidget(self.comboBox)\n",
    "\n",
    "        self.pushButton = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.pushButton.setObjectName(\"pushButton\")\n",
    "        self.pushButton.setText(\"Continue\")\n",
    "        self.pushButton.clicked.connect(self.open_grid_window)\n",
    "        self.verticalLayout.addWidget(self.pushButton)\n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "\n",
    "    def open_grid_window(self):\n",
    "        n = int(self.comboBox.currentText())\n",
    "        self.window = QtWidgets.QMainWindow()\n",
    "        self.ui = Ui_GridWindow(n)\n",
    "        self.ui.setupUi(self.window)\n",
    "        self.window.show()\n",
    "        MainWindow.close()\n",
    "\n",
    "class Ui_GridWindow(object):\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.target_set = False\n",
    "        self.start_set = False\n",
    "        self.block_mode = False\n",
    "        self.grid_buttons = []\n",
    "\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "\n",
    "        self.verticalLayout = QtWidgets.QVBoxLayout(self.centralwidget)\n",
    "        self.verticalLayout.setContentsMargins(30, 30, 30, 30)\n",
    "        self.verticalLayout.setSpacing(20)\n",
    "\n",
    "        self.label_n = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label_n.setText(f\"Grid Size: {self.n} x {self.n}\")\n",
    "        self.label_n.setAlignment(QtCore.Qt.AlignLeft)\n",
    "        self.verticalLayout.addWidget(self.label_n)\n",
    "\n",
    "        self.gridLayoutWidget = QtWidgets.QWidget(self.centralwidget)\n",
    "        self.gridLayout = QtWidgets.QGridLayout(self.gridLayoutWidget)\n",
    "        self.gridLayout.setContentsMargins(20, 20, 20, 20)\n",
    "        self.gridLayout.setSpacing(20)\n",
    "        self.gridLayout.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.gridLayoutWidget.setLayout(self.gridLayout)\n",
    "        self.verticalLayout.addWidget(self.gridLayoutWidget)\n",
    "\n",
    "        # Set the grid layout based on n\n",
    "        for i in range(self.n):\n",
    "            row_buttons = []\n",
    "            for j in range(self.n):\n",
    "                button = QtWidgets.QPushButton(self.gridLayoutWidget)\n",
    "                button.setFixedSize(50, 50)\n",
    "                button.clicked.connect(lambda ch, x=i, y=j: self.grid_button_clicked(x, y))\n",
    "                self.gridLayout.addWidget(button, i, j)\n",
    "                row_buttons.append(button)\n",
    "            self.grid_buttons.append(row_buttons)\n",
    "\n",
    "        self.continueButton = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.continueButton.setText(\"Set Target\")\n",
    "        self.continueButton.clicked.connect(self.set_target_mode)\n",
    "        self.verticalLayout.addWidget(self.continueButton)\n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        \n",
    "        # Calculate the window size\n",
    "        gridSize = self.gridLayout.sizeHint()\n",
    "        windowWidth = gridSize.width() + 100  # Increase padding\n",
    "        windowHeight = gridSize.height() + 100  # Increase padding\n",
    "        MainWindow.setGeometry(100, 100, windowWidth, windowHeight)\n",
    "\n",
    "    def set_target_mode(self):\n",
    "        self.target_set = True\n",
    "        self.continueButton.setText(\"Set Start\")\n",
    "        self.continueButton.clicked.disconnect()\n",
    "        self.continueButton.clicked.connect(self.set_start_mode)\n",
    "\n",
    "    def set_start_mode(self):\n",
    "        self.start_set = True\n",
    "        self.continueButton.setText(\"Set Blocked and Finalize\")\n",
    "        self.continueButton.clicked.disconnect()\n",
    "        self.continueButton.clicked.connect(self.finalize_grid)\n",
    "\n",
    "    def grid_button_clicked(self, x, y):\n",
    "        button = self.grid_buttons[x][y]\n",
    "        if button.text() != \"\" and button.text() != \"Blocked\":  # Allow action only if the cell is empty\n",
    "            return\n",
    "        if not self.target_set:\n",
    "            self.clear_previous(\"Target\")\n",
    "            button.setStyleSheet(\"background-color: red;\")\n",
    "            button.setText(\"Target\")\n",
    "        elif not self.start_set:\n",
    "            self.clear_previous(\"Start\")\n",
    "            button.setStyleSheet(\"background-color: green;\")\n",
    "            button.setText(\"Start\")\n",
    "        elif not self.block_mode:\n",
    "            if button.text() == \"Blocked\":\n",
    "                button.setStyleSheet(\"\")\n",
    "                button.setText(\"\")\n",
    "            else:\n",
    "                button.setStyleSheet(\"background-color: grey;\")\n",
    "                button.setText(\"Blocked\")\n",
    "\n",
    "    def clear_previous(self, text):\n",
    "        for row in self.grid_buttons:\n",
    "            for button in row:\n",
    "                if button.text() == text:\n",
    "                    button.setStyleSheet(\"\")\n",
    "                    button.setText(\"\")\n",
    "\n",
    "    def finalize_grid(self):\n",
    "        grid_representation = []\n",
    "        for i in range(self.n):\n",
    "            row = []\n",
    "            for j in range(self.n):\n",
    "                row.append(self.grid_buttons[i][j].text() if self.grid_buttons[i][j].text() else \"Empty\")\n",
    "            grid_representation.append(row)\n",
    "        \n",
    "        print(\"Final Grid Representation:\")\n",
    "        for row in grid_representation:\n",
    "            print(row)\n",
    "\n",
    "        self.open_algorithm_selection_window(grid_representation)\n",
    "\n",
    "    def open_algorithm_selection_window(self, grid_representation):\n",
    "        self.algorithm_window = QtWidgets.QMainWindow()\n",
    "        self.algorithm_ui = Ui_AlgorithmSelectionWindow(self.n, grid_representation, self.grid_buttons)\n",
    "        self.algorithm_ui.setupUi(self.algorithm_window)\n",
    "        self.algorithm_window.show()\n",
    "\n",
    "class Ui_AlgorithmSelectionWindow(object):\n",
    "    def __init__(self, n, grid_representation, grid_buttons):\n",
    "        self.n = n\n",
    "        self.grid_representation = grid_representation\n",
    "        self.grid_buttons = grid_buttons\n",
    "\n",
    "    def setupUi(self, MainWindow):\n",
    "        MainWindow.setObjectName(\"MainWindow\")\n",
    "        self.centralwidget = QtWidgets.QWidget(MainWindow)\n",
    "        self.centralwidget.setObjectName(\"centralwidget\")\n",
    "\n",
    "        self.verticalLayout = QtWidgets.QVBoxLayout(self.centralwidget)\n",
    "        self.verticalLayout.setContentsMargins(10, 10, 10, 10)\n",
    "        self.verticalLayout.setSpacing(10)\n",
    "        self.verticalLayout.setAlignment(QtCore.Qt.AlignCenter)\n",
    "\n",
    "        self.label = QtWidgets.QLabel(self.centralwidget)\n",
    "        self.label.setText(\"Choose an Option\")\n",
    "        self.label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "        self.verticalLayout.addWidget(self.label)\n",
    "\n",
    "        self.button_qlearning = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.button_qlearning.setText(\"Q-learning\")\n",
    "        self.button_qlearning.clicked.connect(lambda: self.show_final_grid(\"Q-learning\"))\n",
    "        self.verticalLayout.addWidget(self.button_qlearning)\n",
    "\n",
    "        self.button_dyna_q = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.button_dyna_q.setText(\"DynaQ\")\n",
    "        self.button_dyna_q.clicked.connect(lambda: self.show_final_grid(\"DynaQ\"))\n",
    "        self.verticalLayout.addWidget(self.button_dyna_q)\n",
    "\n",
    "        self.button_dyna_q_plus = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.button_dyna_q_plus.setText(\"DynaQ+\")\n",
    "        self.button_dyna_q_plus.clicked.connect(lambda: self.show_final_grid(\"DynaQ+\"))\n",
    "        self.verticalLayout.addWidget(self.button_dyna_q_plus)\n",
    "\n",
    "#changes made for Prioritized Sweeping\n",
    "        self.button_PrioritizedSweeping = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.button_PrioritizedSweeping.setText(\"Prioritized-Sweeping\")\n",
    "        self.button_PrioritizedSweeping.clicked.connect(lambda: self.show_final_grid(\"Prioritized-Sweeping\"))\n",
    "        self.verticalLayout.addWidget(self.button_PrioritizedSweeping)\n",
    "\n",
    "        self.button_compare = QtWidgets.QPushButton(self.centralwidget)\n",
    "        self.button_compare.setText(\"Compare\")\n",
    "        self.button_compare.clicked.connect(lambda: self.show_final_grid(\"Compare\"))\n",
    "        self.verticalLayout.addWidget(self.button_compare)\n",
    "\n",
    "        MainWindow.setCentralWidget(self.centralwidget)\n",
    "        MainWindow.setGeometry(100, 100, 300, 250)\n",
    "\n",
    "    def show_final_grid(self, algorithm):\n",
    "        print(f\"Selected Algorithm: {algorithm}\")\n",
    "        global start\n",
    "        grid_representation = []\n",
    "        blocked_tiles = []\n",
    "        for i in range(self.n):\n",
    "            row = []\n",
    "            for j in range(self.n):\n",
    "                if self.grid_buttons[i][j].text() == \"Blocked\":\n",
    "                    blocked_tiles.append((i, j))\n",
    "                row.append(self.grid_buttons[i][j].text() if self.grid_buttons[i][j].text() else \"Empty\")\n",
    "            grid_representation.append(row)\n",
    "        \n",
    "        # print(\"Final Grid Representation:\")\n",
    "        # for row in grid_representation:\n",
    "        #     print(row)\n",
    "        \n",
    "        maze_env = Grid(self.n, blocked_tiles)\n",
    "        global x_T, y_T  # Declare global variables for the target position\n",
    "        x_T, y_T = None, None\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                if grid_representation[i][j] == \"Start\":\n",
    "                    start = (i, j)\n",
    "                elif grid_representation[i][j] == \"Target\":\n",
    "                    x_T, y_T = i, j\n",
    "        player = Player(maze_env)\n",
    "\n",
    "\n",
    "        if (algorithm == \"Q-learning\"):\n",
    "            player.train_Q()\n",
    "            optimal_path = player.path(start)\n",
    "            self.display_colored_grid(grid_representation, optimal_path)\n",
    "            print(\"Optimal Path (Q-learning):\", optimal_path)\n",
    "            print(\"Number of steps taken by agent to reach target (Q-learning):\", len(optimal_path) - 1)\n",
    "            \n",
    "\n",
    "\n",
    "        elif (algorithm == \"DynaQ\"):\n",
    "            player.train_DynaQ()\n",
    "            optimal_path = player.path(start)\n",
    "            self.display_colored_grid(grid_representation, optimal_path)\n",
    "            print(\"Optimal Path (Dyna-Q):\", optimal_path)\n",
    "            print(\"Number of steps taken by agent to reach target (Dyna-Q):\", len(optimal_path) - 1)\n",
    "            \n",
    "\n",
    "\n",
    "        elif(algorithm == \"DynaQ+\"):\n",
    "            player.train_DynaQPlus()\n",
    "            optimal_path = player.path(start)\n",
    "            self.display_colored_grid(grid_representation, optimal_path)\n",
    "            print(\"Optimal Path (Dyna-Q+):\", optimal_path)\n",
    "            print(\"Number of steps taken by agent to reach target (Dyna-Q+):\", len(optimal_path) - 1)\n",
    "\n",
    "        elif(algorithm == \"Prioritized-Sweeping\"):\n",
    "            player.train_PrioritizedSweeping()\n",
    "            optimal_path = player.path(start)\n",
    "            self.display_colored_grid(grid_representation, optimal_path)\n",
    "            print(\"Optimal Path (PrioritizedSweeping):\", optimal_path)\n",
    "            print(\"Number of steps taken by agent to reach target (PrioritizedSweeping):\", len(optimal_path) - 1)\n",
    "            \n",
    "\n",
    "        else:\n",
    "            player1 = Player(maze_env)\n",
    "            player2 = Player(maze_env)\n",
    "            player3 = Player(maze_env)\n",
    "            player4 = Player(maze_env)\n",
    "            # player1.train_Q()\n",
    "            # optimal_path1 = player1.path(start)\n",
    "            # self.display_colored_grid(grid_representation, optimal_path1)\n",
    "            # player2.train_DynaQ()\n",
    "            # optimal_path2 = player2.path(start)\n",
    "            # self.display_colored_grid(grid_representation, optimal_path2)\n",
    "            # player3.train_DynaQPlus()\n",
    "            # optimal_path3 = player3.path(start)\n",
    "            # self.display_colored_grid(grid_representation, optimal_path3)\n",
    "            # player.reset()\n",
    "            global train_ep\n",
    "            if(self.n == 2) : train_ep = 10\n",
    "            elif(self.n == 3 ) : train_ep = 30\n",
    "            elif(self.n == 4 ) : train_ep = 120\n",
    "            elif(self.n == 5 ) : train_ep = 400\n",
    "            elif(self.n == 6 ) : train_ep = 800\n",
    "            elif(self.n == 7 ) : train_ep = 1100\n",
    "            elif(self.n == 8 ) : train_ep = 1500\n",
    "            elif(self.n == 9 ) : train_ep = 2000\n",
    "            elif(self.n == 10 ) : train_ep = 2500\n",
    "                \n",
    "            player1.train_Q(train_ep)\n",
    "            Q_steps_per_ep = player1.Q_steps_per_ep\n",
    "\n",
    "            # player.reset()\n",
    "            player2.train_DynaQ(train_ep)\n",
    "            DynaQ_steps_per_ep = player2.dynaQ_steps_per_ep\n",
    "\n",
    "            # player.reset()\n",
    "            player3.train_DynaQPlus(train_ep)\n",
    "            DynaQPlus_steps_per_ep = player3.dynaQPlus_steps_per_ep\n",
    "\n",
    "            player4.train_PrioritizedSweeping(train_ep)\n",
    "            PrioritizedSweeping_steps_per_ep = player4.PrioritizedSweeping_steps_per_ep\n",
    "\n",
    "            # Plot the comparison graph\n",
    "\n",
    "            fig, axs = plt.subplots(4, 1, figsize=(10, 15), sharex=True)\n",
    "            \n",
    "            # Define the number of episodes for x-ticks\n",
    "            xticks = range(0, train_ep, int(train_ep/20))\n",
    "            \n",
    "            # Plot Q-learning\n",
    "            axs[0].plot(Q_steps_per_ep, label='Q-learning', color='b')\n",
    "            axs[0].set_ylabel('Number of Steps')\n",
    "            axs[0].set_title('Q-learning')\n",
    "            axs[0].legend()\n",
    "            axs[0].set_xticks(xticks)\n",
    "            \n",
    "            # Plot Dyna-Q\n",
    "            axs[1].plot(DynaQ_steps_per_ep, label='Dyna-Q', color='g')\n",
    "            axs[1].set_ylabel('Number of Steps')\n",
    "            axs[1].set_title('Dyna-Q')\n",
    "            axs[1].legend()\n",
    "            axs[1].set_xticks(xticks)\n",
    "            \n",
    "            # Plot Dyna-Q+\n",
    "            axs[2].plot(DynaQPlus_steps_per_ep, label='Dyna-Q+', color='r')\n",
    "            axs[2].set_ylabel('Number of Steps')\n",
    "            axs[2].set_title('Dyna-Q+')\n",
    "            axs[2].legend()\n",
    "            axs[2].set_xticks(xticks)\n",
    "            \n",
    "            # Plot Prioritized Sweeping\n",
    "            axs[3].plot(PrioritizedSweeping_steps_per_ep, label='Prioritized Sweeping', color='black')\n",
    "            axs[3].set_xlabel('Episodes')\n",
    "            axs[3].set_ylabel('Number of Steps')\n",
    "            axs[3].set_title('Prioritized Sweeping')\n",
    "            axs[3].legend()\n",
    "            axs[3].set_xticks(xticks)\n",
    "            \n",
    "            plt.suptitle('Comparison of Q-learning, Dyna-Q, Dyna-Q+, and Prioritized Sweeping')\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Number of episodes for Q-learning agent to find optimal path:\",find_start_of_optimal_solution(player1.Q_steps_per_ep))\n",
    "            print(\"Number of episodes for Dyna-Q agent to find optimal path:\",find_start_of_optimal_solution(player2.dynaQ_steps_per_ep))\n",
    "            print(\"Number of episodes for Dyna-Q-Plus agent to find optimal path:\",find_start_of_optimal_solution(player3.dynaQPlus_steps_per_ep))\n",
    "            print(\"Number of episodes for Prioritized Sweeping agent to find optimal path:\",find_start_of_optimal_solution(player4.PrioritizedSweeping_steps_per_ep))\n",
    "\n",
    "\n",
    "    def display_colored_grid(self, grid_representation, optimal_path):\n",
    "        self.color_window = QtWidgets.QMainWindow()\n",
    "        self.color_centralwidget = QtWidgets.QWidget(self.color_window)\n",
    "        self.color_gridLayout = QtWidgets.QGridLayout(self.color_centralwidget)\n",
    "        self.color_centralwidget.setLayout(self.color_gridLayout)\n",
    "\n",
    "        color_mapping = {\n",
    "            \"Empty\": \"white\",\n",
    "            \"Start\": \"green\",\n",
    "            \"Blocked\": \"black\",\n",
    "            \"Target\": \"red\",\n",
    "            \"Path\": \"yellow\"\n",
    "        }\n",
    "\n",
    "        for i in range(self.n):\n",
    "            for j in range(self.n):\n",
    "                tile_type = grid_representation[i][j]\n",
    "                tile_color = color_mapping[tile_type]\n",
    "                tile_label = QtWidgets.QLabel(self.color_centralwidget)\n",
    "                tile_label.setStyleSheet(f\"background-color: {tile_color}; border: 1px solid black;\")\n",
    "                tile_label.setFixedSize(50, 50)\n",
    "                if tile_type != \"Empty\":\n",
    "                    tile_label.setText(tile_type)\n",
    "                    tile_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "                self.color_gridLayout.addWidget(tile_label, i, j)\n",
    "\n",
    "        # Highlight the optimal path\n",
    "        for x, y in optimal_path[1:-1]:\n",
    "            tile_label = QtWidgets.QLabel(self.color_centralwidget)\n",
    "            tile_label.setStyleSheet(\"background-color: yellow; border: 1px solid black;\")\n",
    "            tile_label.setFixedSize(50, 50)\n",
    "            tile_label.setText(\"Path\")\n",
    "            tile_label.setAlignment(QtCore.Qt.AlignCenter)\n",
    "            self.color_gridLayout.addWidget(tile_label, x, y)\n",
    "\n",
    "        self.color_window.setCentralWidget(self.color_centralwidget)\n",
    "        self.color_window.setGeometry(100, 100, self.n * 50 + 40, self.n * 50 + 40)\n",
    "        self.color_window.show()\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    app = QtWidgets.QApplication(sys.argv)\n",
    "    MainWindow = QtWidgets.QMainWindow()\n",
    "    ui = Ui_MainWindow()\n",
    "    ui.setupUi(MainWindow)\n",
    "    MainWindow.show()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a89d35-1117-4934-a97b-6895fc046738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0555c9-4f65-4af6-a8b3-1cd04a862f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
